% Non-parametric approaches to optimal policy are more robust



```{r knit-settings, cache=FALSE}
options(xtable.print.comment=FALSE)
options(xtable.type = 'latex', table.placement="H")
opts_chunk$set(warning=FALSE, message=FALSE, comment=NA, tidy=FALSE,
 echo=FALSE)
opts_knit$set(progress = TRUE, verbose = TRUE)
opts_chunk$set(dev = 'Cairo_pdf', fig.width=5.5, fig.height=4,
 cache.path = 'cache-pdf/', cache=TRUE)
```



```{r libraries, include=FALSE}
library(pdgControl)
library(nonparametricbayes)
library(reshape2)
library(ggplot2)
library(data.table)
library(tgp)
library(MCMCpack)
library(plyr)
library(knitcitations)
```
```{r plotting-options, echo=FALSE, include=FALSE}
theme_set(theme_bw(base_size=10))
theme_update(panel.background = element_rect(fill = "transparent",colour = NA),
             plot.background = element_rect(fill = "transparent",colour = NA))
cbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```



Abstract
=======================================================================


Introduction
=======================================================================


1. Classical approaches are to fix relationship of stock recruitment.  
2. Parameter uncertainty
3. Gaussian processes for model uncertainty


The problem of structural uncertainty in managing ecological systems.

(Big literature here of course, primary approaches have been using
parametric uncertainty and model uncertainty (e.g. belief distribution over models 
I believe. Worth distinguishing our problem from the more trivially 
sized problems in which the action space is discrete and consists
of 2 or three possible choices, e.g. much of the recent Possingham et al work.)


Most management recommendations from the ecological literature are based on
(or at least motivated by) parametric models. Though in principle these models
reflect mechanistic underpinnings of biological interactions involved, in practice
real mechanisms are often unknown and the true dynamics too complex to be captured
by simple models [@refs]. While simple mechanistic models can nevertheless
provide imporant insights into possible dynamics -- for instance, demonstrating
that a vaccine does not have to be 100% effective to eliminate the transmission of a virus [@Kermack1921] -- such approaches are not well-suited for use in 
forecasting future dynamics upon which outcomes management policy can be based.  
Despite a long history of work emphasizing the difference between modeling 
for understanding (generic or strategic models) and modeling for prediction (precise, specific, or tactical models) [e.g. @Levins1966], simple parametric models 
continue to find application in predictive management contexts.  The management
of marine fisheries is a prime example, in which simple, mechanistically 
motivated models of stock-recruitment relationships such as Ricker or Beverton-Holt
curves are systematically fit to data and used in solving decision-theoretic
or optimal control problems determining resource management policies such
as maximum sustainable yield [@refs].  

Stochastic dynamic programming @Mangel1982

To address structural uncertainty in the processes involved, such mechanistic models
can be adapted to accomidate uncertainty in the parameters or consider a set of 
alternative models simultaneously (belief SDP) 


Background on the concerns of structural uncertainty -- we don't have
the right models.  Further unknonws such as measurement uncertainty,
parameter uncertainty, unobserved states, knowledge of boundary
conditions, etc. further compound the issue.  Though a hierarchical
Bayesian approach provides a natural way to address these from a
statistical standpoint, formulating reasonable parametric descriptions
of each form of uncertainty is a challenging task in itself, let alone
the computational difficulties of solving such a system. @Cressie2009
provides a good account of the successess and challenges of the approach. 
Applying these approaches in the management context of sequential decision
making, in which forecasts must be obtained over a range of possible actions
and updated regularly as new information arrives makes such an approach 
less feasible still.  

* Management goals / decision-theoretic approaches need accurate
prediction over relevant (short?) timescales more than accurate (but
incomplete or noisy estimated) mechanisms.


* Nonparametric (machine-learning?) approaches may offer the benefit of
the hierarchical Bayesian approach without the practical and computational
limitations of their parametric kin.  Non-parametric models are flexible
enough to take maximum advantage of the data available, while being
appropriately ambiguous about the dynamics of a system in regions of
parameter space that have been poorly or never sampled.

Though machine learning approaches have begun to appear in the
ecological and conservation literature (Species distribution models),
including the Gaussian process based approach used here [@Munch2005],
they remain unfamiliar and untrusted approaches for most ecologists.
Machine learning approaches represent an essentially pattern-based
rather than process-based approach, raising the same skepticism from most
theoretical ecologists that they hold for older correlative methods such
as linear regression, while the complexity of the techniques has barred
their adoption in more empirical audiences. <!-- Whoa! rampant speculation
alert! -->


.  However, such approaches have yet to be applied to the
decision-theoretic framework that could guide management decisions,
where we expect them to excel for several reasons (1) the ability to
make accurate forecasts by more closely approximating the underlying
process where data is available (2) remaining appropriately ambiguous
where data is not available (3) remaining computationally simple enough
to avoid some pitfalls common to hierarchical parametric approaches.




Approach and Methods
=====================================================================


```{r stateeq}
f <- RickerAllee
p <- c(2, 10, 5) 
K <- 10
allee <- 5
```

```{r sdp-pars, dependson="stateeq"}
sigma_g <- 0.05
sigma_m <- 0.0
z_g <- function() rlnorm(1, 0, sigma_g)
z_m <- function() 1+(2*runif(1, 0,  1)-1) * sigma_m
x_grid <- seq(0, 1.5 * K, length=101)
h_grid <- x_grid
profit <- function(x,h) pmin(x, h)
delta <- 0.01
OptTime <- 20  # stationarity with unstable models is tricky thing
reward <- 0
xT <- 0
seed_i <- 1
Xo <- K # observations start from
x0 <- Xo # simulation under policy starts from
Tobs <- 35
```


### Background on Gaussian Process inference

The use of Gaussian process regression (known as Kreging in the geospatial literature) to formulate a predictive model is relatively new in the context of modeling dynamical systems [@Kocijan2005] and introduced in the ecological modeling and fisheries management by @Munch2005.  An accessible and thorough introduction to the formulation and use of Gaussian processes can be found in @Rasmussen2006.  

The essense of the Gaussian process approach can be captured in the following thought experiment: An exhaustive parametric approach to the challenge of structural uncertainty might proceed by writing down all possible functional forms for the underlying dynamical system with all possible parameter values for each form, and then consider searching over this huge space to select the most likely model and parameters; or using a Bayesian approach, assign priors to each of these possible models and infer the posterior distribution of possible models. The Gaussian process approach can be thought of as a computationally efficient approximation to this approach. Gaussian processes represent a large class of models that can be though of as capturing or reasonably approximating the set of models in this collection.  By modeling at the level of the process, rather than the level of parametric equation, we can more concisely capture the possible behavior of these curves.  In place of a parametric model of the dynamical system, the Gaussian Process approach postulates a prior distribution of (n-dimensional) curves that can be though of as approximations to a range of possible (parametric) models that might describe the data. The GP allows us to consider a set of possible curves simultaneously (Figure 1).  <!-- Figure of curves drawn from the posterior density? -->


<!-- Do we need more specifics on Gaussian process as an approximation to parametric models? Discussion of Gaussian process vs other machine learning / forecasting approaches that have less clear statistical foundations?  If so, does this all belong in the discussion? -->

### The optimal control problem


### Discussion on how we compare performance of policies

* Replicate stochastic simulations 
* Sensitivity analysis (Figure 4).  


## Example in a bistable system

Concerns over the potential for tipping points in ecological dynamics
[@Scheffer2001] highlight the dangers of uncertainty in ecological
management and pose a substantial challenge to existing decision-theoretic
approaches [@Brozovic2011].  To compare the performance of nonparametric
and parametric approaches in an example that is easy to conceptualize,
we will focus on a simple parametric model for a single species [derived
from fist principles by @Allen2005a] as our underlying "reality".

\begin{align}
X_{t+1} &= Z_t f(S_t) \\
S_t &= X_t - h_t \\
f(S_t) &= e^{r \left(1 - \frac{S_t}{K}\right)\left(S_t - C\right)}
\end{align}

Where $Z_t$ is multiplicative noise function with mean 1, representing
stochastic growth. We will consider log-normal noise with shape parameter
$\sigma_g$.  We start with an example in which the parameters are $r =$
`r p[1]`, $K =$ `r p[2]`, $C =$ `r p[3]`, and  $\sigma_g =$ `r sigma_g`.


As a low-dimensional system completely described by three parameters, this
scenario should if anything be favorable to a parametric-based approach.
This model contains an Allee effect, or tipping point, below which the
population is not self-sustaining and shrinks to zero [@Courchamp2008].



## Sample training data

Both parametric and nonparametric approaches will require training
data on which to base their model of the process.  We generate the
training data under the model described in Eq 1 for `r Tobs` time
steps, under a known but not necessarily optimal sequence of harvest
intensities, $h_t$.  For simplicity we imagine a fishery that started
from zero harvest pressure and has been gradually increasing the harvest.

<!--Should we include any emprical examples? -->
Using data simulated from a specified model rather than empirical data
permits the comparison against the true underlying dynamics, setting 
a bar for the optimal performance possible.  

(Motivation, alternatives, stationarity, examples without a stable node
(limit-cycle models), examples based on observations near a stable node
alone, and why that isn't impossible).


Results
====================================================

### Discussion of maximum likelihood estimated models

We estimate two parametric models from the data using a maximum likelihood
approach.  The first model is structurally identical to the true model
(Eq 1), differing only in that it's parameters are estimated from the
observed data rather than given.  The alternative model is the Ricker
model, which is structurally similar and commonly used in for such data.  



```{r obs, dependson="sdp-pars"}
  obs <- sim_obs(Xo, z_g, f, p, Tobs=Tobs, nz=15, 
                 harvest = sort(rep(seq(0, .5, length=7), 5)), seed = seed_i)
```



```{r mle, dependson="obs"}
alt <- par_est(obs,  init = c(r = p[1], 
                              K = mean(obs$x[obs$x>0]), 
                              s = sigma_g))
est <- par_est_allee(obs, f, p,  
                     init = c(r = p[1] + 1, 
                              K = p[2] + 2, 
                              C = p[3] + 2, 
                              s = sigma_g))
```



(MLE models will assume the noise is log-normal, which it is in the
simulation).


Which estimates a Ricker model with $r =$ `r alt$p[1]`, $K =$ 
`r alt$p[2]`, and the Allen Allele model with $r =$ `r est$p[1]`, $K =$
`r est$p[2]` and $C =$ `r est$p[3]`.




```{r gp-priors}
#inv gamma has mean b / (a - 1) (assuming a>1) and variance b ^ 2 / ((a - 2) * (a - 1) ^ 2) (assuming a>2)
s2.p <- c(5,5)  
tau2.p <- c(5,1)
d.p = c(10, 1/0.1, 10, 1/0.1)
nug.p = c(10, 1/0.1, 10, 1/0.1) # gamma mean
s2_prior <- function(x) dinvgamma(x, s2.p[1], s2.p[2])
tau2_prior <- function(x) dinvgamma(x, tau2.p[1], tau2.p[2])
d_prior <- function(x) dgamma(x, d.p[1], scale = d.p[2]) + dgamma(x, d.p[3], scale = d.p[4])
nug_prior <- function(x) dgamma(x, nug.p[1], scale = nug.p[2]) + dgamma(x, nug.p[3], scale = nug.p[4])
beta0_prior <- function(x, tau) dnorm(x, 0, tau)
beta = c(0)
priors <- list(s2 = s2_prior, tau2 = tau2_prior, beta0 = dnorm, nug = nug_prior, d = d_prior, ldetK = function(x) 0)
```

```{r gp, dependson=c("obs", "gp-priors")}
  gp <- bgp(X=obs$x, XX=x_grid, Z=obs$y, verb=0,
          meanfn="constant", bprior="b0", BTE=c(2000,16000,2),
          m0r1=FALSE, corr="exp", trace=TRUE, 
          beta = beta, s2.p = s2.p, d.p = d.p, nug.p = nug.p, tau2.p = tau2.p,
          s2.lam = "fixed", d.lam = "fixed", nug.lam = "fixed", tau2.lam = "fixed")      
```




```{r opt, dependson=c("gp", "mle")}
  OPT <- optimal_policy(gp, f, est$f, alt$f,
                        p, est$p, alt$p,
                        x_grid, h_grid, sigma_g, 
                        sigma_g, sigma_g, # est$sigma_g, alt$sigma_g, but those ests are poor
                        delta, xT, profit, reward, OptTime)
```

```{r sim, dependson="opt"}
dt <- simulate_opt(OPT, f, p, x_grid, h_grid, x0, z_g, profit)
```

## Figure 1: 

_Shows the inferred Gaussian Process compared to the true and parametric
models.  Refer to the appendix for details on the GP posteriors, etc._

```{r gp_plot, dependson=c("gp", "mle", "plotting-options"), fig.cap="Graph of the inferred Gaussian process compared to the true process and maximum-likelihood estimated process.  Graph shows the expected value for the function $f$ under each model.  Two standard deviations from the estimated Gaussian process covariance with (light grey) and without (darker grey) measurement error are also shown.  The training data is also shown as black points.  (The GP is conditioned on 0,0, shown as a pseudo-data point). "}
tgp_dat <- 
    data.frame(  x = gp$XX[[1]], 
                 y = gp$ZZ.km, 
                 ymin = gp$ZZ.km - 2 * sqrt(gp$ZZ.ks2), 
                 ymax = gp$ZZ.km + 2 * sqrt(gp$ZZ.ks2),
                 ymin2 = gp$ZZ.mean - 2 * sqrt(gp$ZZ.vark), 
                 ymax2 = gp$ZZ.mean + 2 * sqrt(gp$ZZ.vark))
  true <- sapply(x_grid, f, 0, p)
  alt_mean <- sapply(x_grid, alt$f, 0, alt$p)
  est_mean <- sapply(x_grid, est$f, 0, est$p)
  models <- data.frame(x=x_grid, GP=tgp_dat$y, 
                       Parametric=est_mean, 
                       True=true, 
                       Structural=alt_mean)
  models <- melt(models, id="x")
  names(models) <- c("x", "method", "value")

ggplot(tgp_dat) + 
  geom_ribbon(aes(x,y,ymin=ymin,ymax=ymax), fill="gray80") +
  geom_ribbon(aes(x,y,ymin=ymin2,ymax=ymax2), fill="gray60") +
  geom_line(data=models, aes(x, value, col=method), alpha=0.8, lwd=1) + 
  geom_point(data=obs, aes(x,y)) + 
  xlab(expression(X[t])) + ylab(expression(X[t+1])) +
  scale_colour_manual(values=cbPalette) 
```


## Figure 2: 

_The take-home message, showing that the GP is closest to the optimal
strategy, while the parametric methods are less accurate.  Visualizing the
policy may be more useful for the technical reader, the general audience
may prefer Figure 3 showing all replicates of the population collapse
under the parametric model and not under the GP._

```{r policies_plot, dependson=c("opt", "plotting-options"), fig.cap="The steady-state optimal policy (infinite boundary) calculated under each model.  Policies are shown in terms of target escapement, $S_t$, as under models such as this a constant escapement policy is expected to be optimal [@Reed1979]."}
policies <- 
  melt(data.frame(stock=x_grid, 
                  GP = x_grid[OPT$gp_D], 
                  Parametric = x_grid[OPT$est_D],
                  True = x_grid[OPT$true_D],
                  Structural = x_grid[OPT$alt_D]),
       id="stock")
names(policies) <- c("stock", "method", "value")

ggplot(policies, aes(stock, stock - value, color=method)) +
    geom_line(alpha=0.7, lwd=1) + 
    xlab("stock size") + ylab("escapement")  +
    scale_colour_manual(values=cbPalette)
```

## Figure 3: 

_Figure 3 is a less abstract and more visceral visualization of the
take-home message, with the structurally inaccurate model leading
universally to a collapse of the fishery and very few profits, while the
Gaussian process performs nearly optimally.  The parametric approach even
with the correct underlying structure does not perform optimally, choosing
in this case to under-fish (may need to show harvest dynamics since
that is not clear from the figure! Also isn't general, sometimes does
optimally, sometimes over-fishes.  Perhaps need to show more examples.)
May need to show profits too?_

```{r sim_plot, dependson=c("sim", "plotting-options"), fig.cap="Gaussian process inference outperforms parametric estimates. Shown are 100 replicate simulations of the stock dynamics (eq 1) under the policies derived from each of the estimated models, as well as the policy based on the exact underlying model."}
ggplot(dt) + 
    geom_line(aes(time, fishstock, 
                  group=interaction(reps, method), 
                  color=method), alpha=.1) +
    scale_colour_manual(values=cbPalette, 
                        guide = guide_legend(override.aes = list(alpha = 1)))
```

## Figure 4:

_Shows the sensitivity analysis.  A histogram of distribution of yield
over stochastic realizations, showing that the qualitative results do
not depend on the stochastic realization of the training data here, or on
the parameters of the underlying model, though quantitative differences
are visible._



Discussion / Conclusion
==================================================================




* Non-parametric methods have received far too little attention in
ecological modeling efforts that are aimed at improved conservation
planning and decision making support.

* Importance of non-parametric approaches in conservation planning /
resource management / decision theory.

* Decision-theoretic tools such as optimal control calculations rely
on robust _forecasting_ more strongly than they rely on accurate
_mechanistic_ relationships.

* Adapting a non-parametric approach requires modification of existing
methods for decision theory.  We have illustrated how this might be
done in the context of stochastic dynamic programming, opening the
door for substantial further research into how these applications might
be improved.

* Anticipate improved relative performance in higher dimensional examples

* Discuss constant escapement in model, in policies.

* Limitations of this comparison: Are the maximum-likelihood solutions
a straw man?

* Discussion of alternative related approaches: POMDP/MOMDP,

Future directions
--------------------------------------------------------------------

* Multiple species * Online learning * Multiple step-ahead predictions *
Explicitly accomidating additional uncertainties * Improving inference
of optimal policy from the GP

```{r echo=FALSE, results="asis"} #bibliography("html") ```


Acknowledgments
================

This work was partially supported by the Center for Stock Assessment Research, a partnership between the University of California Santa Cruz and the Fisheries Ecology Division, Southwest Fisheries Science Center, Santa Cruz, CA and by NSFÂ grant EF-0924195 to MM.

Appendix / Supplementary Materials
==================================

### The Guassian process

The Gaussian process is defined by a covariance kernel.  By requiring our kernel to follow a generic functional form, we can compactly describe the Gaussian process using only a handful of parameters (Table 1) 


parameter       interpretation
---------       -------------- 
$\sigma^2$      The process noise (from the kernel)
$\tau^2$        Variance around the mean
$\beta$_0       The mean is given by a linear model of slope $\beta$ 
$d$             The length-scale of the covariance function
$n$             The observation error

: Table of parameters for the Gaussian process

Rather than estimate values of these parameters directly, we take a hierarchical approach of placing prior distributions on each.  Following @Gramarcy2005 we use a Gaussian prior on $\beta_0$, an inverse gamma prior on $\sigma^2$ and $\tau^2$, a gamma prior on the observation noise $n$, and exponential prior on the length-scale $d$. 

###  Formulating a dynamic programming solution

The fishery management problem over an infinite time horizon can be stated as:

\begin{align}
& \max_{ \{h_t\} \geq 0 } \mathbf{E} \lbrace \sum_0^\infty \delta^t \Pi(h_t) \rbrace \\
& \mathrm{s.t.}  \\
 & X_t = Z_t f\left(S_{t-1}\right) \\
 & S_t = X_t - h_t \\
 & X_t  \geq 0 
\end{align}

Where $\mathbf{E}$ is the expectation operator, $\delta$ the discount
rate, $\Pi(h_t)$ the profit expected from a harvest of $h_t$, and other
terms as in Eq. (1).  For simplicity, we have assumed that profits depend
only on the chosen harvest; simplifying further we will usually consider
profits to be proportional to harvest, $\Pi(h_t) = h_t$.

Once the posterior Gaussian process (GP) has been estimated [e.g. see
@Munch2005], it is necessary to adapt it in place of the parametric
equation for the stochastic dynamic programming (SDP) solution [see
@Mangel1988 for a detailed description of parametric SDP methods] to the
optimal policy. The essense of the idea is straight forward -- we will use
the estimated GP in place of the parametric growth function to determine
the stochastic transition matrix on which the SDP calculations are based.

The posterior Gaussian process is completely defined by the expected value
and covariance matrix at a defined set of training points.  For simplicty
we will consider a these points to fall on a discrete, uniform grid $x$
of `r length(x_grid)` points from `r min(x_grid)` to `r max(x_grid)`
(1.5 times the positive equilibrium $K$).  Again to keep things simple
we will use this same grid discritization for the parametric approach.
Other options for choosing the grid points, including collocation methods
and functional basis expansion (or even using Guassian processes in place
of the discrete optimization; an entirely different context in which GP
can be used in SDP, see [@Deisenroth2009]) could also be considered.

The transition matrix $\mathbf{F}$ is thus an `r length(x_grid)`
by `r length(x_grid)` matrix for which the ${i,j}$ entry gives the
probability of transitioning into state $x_i$ given that the system is
in state $x_j$ in the previous timestep.  To generate the transition
matrix based on the posterior GP, we need only the expected values
at each grid point and the corresponding variances (the diagonal of
the covariance matrix), as shown in Figure 1.  Given the mean at each
gridpoint as the length `r length(x_grid)` vector $E$ and variance $V$,
the probability of transitioning from state $x_i$ to state $x_j$ is
simply $\mathcal{N}\left(x_j | \mu = E_i, \sigma = \sqrt{V_i}\right)$,
where $\mathcal{N}$ is the Normal density at $x_j$ with mean $\mu$ and
variance $\sigma^2$.  Strictly speaking, the transition probability should
be calculated by integrating the normal density over the bin of width
$\Delta$ centered at $x_j$.  For a sufficiently fine grid that $f(x_j)
\approx f(x_j + \Delta)$, it is sufficient to calculate the density at
$x_j$ and then row-normalize the transition matrix.



## Pseudocode for the determining the transtion matrix from the GP

```r
for(h in h_grid)
  F_h = for(x_j in grid)
          for(i in 1:N) 
            dnorm(x_j, mu[i]-h, V[i])
```


A transition matrix for each of the parametric models $f$ is calculated
using the log-normal density with mean $f(x_i)$ and log-variance as
estimated by maximum likelihood.  From the discrete transition matrix we
may write down the Bellman recursion defining the the stochastic dynamic
programming iteration:

\begin{equation}
V_t(x_t) = \max_h \mathbf{E} \left( h_t + \delta V_{t+1}( Z_{t+1} f(x_t - h_t)) \right)
\end{equation}

where $V(x_t)$ is the value of being at state $x$ at time $t$, $h$
is control (harvest level) chosen. Numerically, the maximization is
accomplished as follows. Consider the set of possible control values to
be the discrete `r length(h_grid)` values corresponding the the grid of
stock sizes.  Then for each $h_t$ there is a corresponding transition
matrix $\mathbf{F}_h$ determined as described above but with mean 
$\mu = x_j - h_t$. Let $\vec{V_t}$ be the vector whose $i$th element corresponds
to the value of having stock $x_i$ at time $t$.  Then let $\Pi_h$ be
the vector whose $i$th element indicates the profit from harvesting
at intensity $h_t$ given a population $x_i$ (e.g. $\max(x_i, h_t)$
since one cannot harvest more fish then the current population size).
Then the Bellman recursion can be given in matrix form as

$$V_{t} = \max_h \left( \Pi_{h_{t}} + \delta \mathbf{F}_h V_{t+1} \right)$$

where the sum is element by element and the expectation is computed by the matrix multiplication $\mathbf{F} V_{t+1}$.  

### Pseudocode for the Bellman iteration

```r
 V1 <- sapply(1:length(h_grid), function(h){
      delta * F[[h]] %*% V +  profit(x_grid, h_grid[h]) 
    })
    # find havest, h that gives the maximum value
    out <- sapply(1:gridsize, function(j){
      value <- max(V1[j,], na.rm = T) # each col is a diff h, max over these
      index <- which.max(V1[j,])  # store index so we can recover h's 
      c(value, index) # returns both profit value & index of optimal h.  
    })
    # Sets V[t+1] = max_h V[t] at each possible state value, x
    V <- out[1,]                        # The new value-to-go
    D[,OptTime-time+1] <- out[2,]       # The index positions
```

_Currently this shows the literal R code, should be adapted_ 


MCMC posterior distributions and convergence analysis
----------------------------------------------------------------------------

```{r posteriors, dependson=c("gp", "plotting-options"), fig.cap = "Histogram of posterior distributions for the estimated Gaussian Process shown in Figure 1.  Prior distributions overlaid."}
hyperparameters <- c("index", "s2", "tau2", "beta0", "nug", "d", "ldetK")
posteriors <- melt(gp$trace$XX[[1]][,hyperparameters], id="index")
  prior_curves <- ddply(posteriors, "variable", function(dd){
  grid <- seq(min(dd$value), max(dd$value), length = 100)
  data.frame(value = grid, density = priors[[dd$variable[1]]](grid))
})

ggplot(posteriors) + 
    geom_histogram(aes(x=value, y=..density..), alpha=0.7) +
    geom_line(data=prior_curves, aes(x=value, y=density), col="red") +
    facet_wrap(~ variable, scale="free")
```
 
 @Gramacy2005
 
## Tables of nuisance parameters, sensitivity analysis

### List of hyper-parameters, prior distributions and their parameters

## Reproducible code, "Research Compendium"



